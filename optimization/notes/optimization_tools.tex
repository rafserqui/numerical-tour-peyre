%========================================================================================
% Compilation should work with PDFLaTeX
%========================================================================================
% Type of document and general formatting
\documentclass[a4paper,11pt]{article}

\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\linespread{1.25}

%========================================================================================
% These packages are for language and font settings
\usepackage[english,activeacute]{babel} % Language
\usepackage{tgpagella}					% Text font
\usepackage[T1]{fontenc}				% T1 Encoding of font
\usepackage[utf8x]{inputenc}				% Special symbols
\usepackage{lmodern}


%========================================================================================
\usepackage[sc]{mathpazo}				% Math font
\usepackage{amsmath,amsfonts,amssymb}	% Math symbols
\usepackage{dsfont}						% Math symbols like R for reals...


%========================================================================================
% Other packages
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[svgnames]{xcolor}

%========================================================================================
\usepackage{accents}
\newcommand*{\dt}[1]{%
	\accentset{\mbox{\large\bfseries .}}{#1}} % Larger dot for time derivative


\usepackage{hyperref}
\hypersetup
{
    pdfauthor={Rafael Serrano-Quintero},
    pdfsubject={Structural Transformation in Indian Services},
    colorlinks = {true},
    linkcolor = {FireBrick},
    citecolor = {FireBrick},
    urlcolor = {RoyalBlue},
}

\usepackage{appendix}
\usepackage{marvosym}
\usepackage{enumerate} %For enumerating with letters with option [a)]
\usepackage{fancyvrb}  %To reduce font size in verbatim environment
\usepackage{epstopdf}
\usepackage[flushleft]{threeparttable}
\usepackage{pdflscape}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[super]{nth}
\usepackage{float}

\newcommand{\source}[1]{\caption*{\tiny Source: {#1}} }

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\dotprod}[2]{\langle #1, #2 \rangle}
\newcommand{\diver}{\text{div}}

%========================================================================================
% Stata Preamble for Tables
%========================================================================================

\newcommand{\sym}[1]{\rlap{#1}}% Thanks to David Carlisle

\let\estinput=\input% define a new input command so that we can still flatten the document

\newcommand{\estwide}[3]{
		\vspace{.75ex}{
			\begin{tabular*}
			{\textwidth}{@{\hskip\tabcolsep\extracolsep\fill}l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
			\end{tabular*}
			}
		}	

\newcommand{\estauto}[3]{
		\vspace{.75ex}{
			\begin{tabular}{l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
			\end{tabular}
			}
		}

% Allow line breaks with \\ in specialcells
	\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% *****************************************************************
% Custom subcaptions
% *****************************************************************
% Note/Source/Text after Tables
\newcommand{\figtext}[1]{
	\vspace{-1.9ex}
	\captionsetup{justification=justified,font=footnotesize}
	\caption*{\hspace{6pt}\hangindent=1.5em #1}
	}
\newcommand{\fignote}[1]{\figtext{\emph{Note:~}~#1}}

\newcommand{\figsource}[1]{\figtext{\emph{Source:~}~#1}}

% Add significance note with \starnote
\newcommand{\starnote}{\figtext{* p < 0.1, ** p < 0.05, *** p < 0.01. Standard errors in parentheses.}}

% *****************************************************************
% siunitx
% *****************************************************************
\usepackage{siunitx} % centering in tables
	\sisetup{
		detect-mode,
		tight-spacing		= true,
		group-digits		= false ,
		input-signs		= ,
		input-symbols		= ( ) [ ] - + *,
		input-open-uncertainty	= ,
		input-close-uncertainty	= ,
		table-align-text-post	= false
        }

%========================================================================================
					% === Title, thanks, and author data === %
%========================================================================================


\title{\textbf{Optimization Methods}\thanks{These are notes based on Gabriel Peyr\'e's exceptional \href{http://www.numerical-tours.com/matlab/}{Numerical Tours} and are for my own personal study.}}

\begin{document}
\maketitle

\section{Gradient Descent Methods}

\subsection{Gradient Descent for Unconstrained Problems}

We consider the problem of finding a minimum of a function $f$, hence solving

\[
\underset{x\in\mathbb{R}^d}{\text{min}} f(x)
\]

where $f : \mathbb{R}^d \mapsto \mathbb{R}$ is a smooth function.

The minimum is not necessarily unique. In the general case, $f$ might exhibit local minima, in which case the proposed algorithms are not expected to find a global minimizer of the problem. In this tour, we restrict our attention to convex function, so that the methods will converge to a global minimizer.

The simplest method is the gradient descent, that computes

\[
x^{(k+1)} = x^{(k)} - \tau_k \nabla f\left(x^{(k)}\right)
\]

where $\tau_k > 0$ is a step size, and $\nabla f(x) \in\mathbb{R}^d$ is the gradient of $f$ at the point $x$, and $x^{(0)}\in\mathbb{R}^d$ is an initial point.

In the convex case, if $f$ is of class $\mathcal{C}^2$, in order to ensure convergence, the step size should satisfy 

\[
0 < \tau_k < \frac{2}{\text{sup}_x \norm{H f(x)}}
\]

where $Hf(x)\in\mathbb{R}^{d\times d}$ is the Hessian of $f$ at $x$ and $\norm{\cdot}$ is the spectral operator norm (largest eigenvalue). 

The following code takes $f(x) = x^2$, computes the gradient manually and applies  gradient descent to get to the solution $x = 0$.

\begin{Verbatim}[fontsize=\small]
tau = 2e-1; 			% Step-size parameter
f = @(x)(x.^2);			% Function to minimize
fgrad = @(x)(2.*x);		% Gradient

x0 = -500;		% Initial guess
tol = 1e-6;		% Tolerance of the algorithm
err = 10000;	% Initial error
it = 1;			% Iteration counter

while err > tol
	fgradx0 = fgrad(x0);
	x1(it+1) = x0-tau.*fgradx0;
	err = abs(fgradx0); % Error is the absolute value of the gradient
	if err > tol
		x0 = x1(it+1);
	end
	fprintf('New x = %3.3f \n',x0)
	it = it+1;
end
\end{Verbatim}

\subsection{Gradient Descent in 2-D}
Suppose we want to minimize the following quadratic form

\[
f(x) = \frac{1}{2}\left(x_1^2 + \eta x_2^2\right)
\]

where $\eta$ controls the anisotropy and, hence, the difficulty of the problem.\footnote{Anisotropy is the property of being directionally dependent, which implies different properties in different directions, as opposed to isotropy.} Let us set $\eta = 10$.

The rationale for the code is the same as before. However, now I impose two stopping conditions, one for each coordinate. Note that the step-size parameter $\tau_k$ needs to be smaller than $2/\eta$. Figure

\begin{figure}[htbp]
	\centering 
		\includegraphics[width = 0.35\textwidth]{../figures/gradient_descent_2D.pdf}
	\caption{Visualization of Gradient Descent in $2-$D for Different Values of $\tau_k$}
	\label{fig:gradient_descent_2d}
\end{figure}

\subsection{Gradient and Divergence of Images}

Local differential operators like gradient, divergence and laplacian are the building blocks for variational image processing.

\paragraph{Gradient} An image is a matrix $x_0\in\mathbb{R}^N$ of $N=n\times n$ pixels. For a continuous function $g$, the gradient reads

\[
\nabla g(s) = \left(\frac{\partial g(s)}{\partial s_1},\frac{\partial g(s)}{\partial s_2}\right)\in\mathbb{R}^2
\]
(note that here, the variable $s$ denotes the $2-$D spatial position).

We discretize this differential operator on a discrete image $x\in\mathbb{R}^N$ using first order finite differences.

\[
\left(\nabla x\right)_i = (x_{i_1,i_2} - x_{i_1 - 1, i_2}, x_{i_1,i_2} - x_{i_1,i_2-1})\in\mathbb{R}^2
\]

Note that for simplity we use periodic boundary conditions. Thus, we get $\nabla : \mathbb{R}^n\mapsto\mathbb{R}^{N\times 2}$. Figure \ref{fig:grad_image} shows the discretized gradient in two images. The first row of figures shows a simple graph with two squares. Notice how the $dx$ shows a vertical lines denoting changes from black into white and the other way around, while the $dy$ figure shows horizontal lines denoting the change from black into white. The second row shows the same but for a real image.

\begin{figure}[htbp]
	\centering
		\includegraphics[width = 0.65\textwidth]{../figures/gradient_descent_image.pdf}
	\caption{Gradient Descent in a 2-D Image}
	\label{fig:grad_image}
\end{figure}

\paragraph{Magnitude} The magnitude is just defined as the Euclidean norm, i.e. $\norm{\nabla g} = \sqrt{\left(\frac{\partial g(s)}{\partial s_1}\right)^2+\left(\frac{\partial g(s)}{\partial s_2}\right)^2}$. The magnitude is larger closer to the edges of the image. Figure \ref{fig:norm_image} shows the Euclidean norm or the magnitude for the image. This highlights borders where changes of color or transparency are sharper.

\begin{figure}[htbp]
	\centering
		\includegraphics[width = 0.65\textwidth]{../figures/magnitude_image.pdf}
	\caption{Magnitude in a 2-D Image}
	\label{fig:norm_image}
\end{figure}

\paragraph{Divergence Operator} The divergence operator maps vector field to images. For continuous vector fields $v(s)\in\mathbb{R}^2$, it is defined as 
\[
\diver(v)(s) = \frac{\partial v_1(s)}{\partial s_1} + \frac{\partial v_2(s)}{\partial s_2} \in\mathbb{R}
\]

(note that here, the variable $s$ denotes the 2-D spatial position). It is minus the adjoint of the gadient, i.e. $\diver=âˆ’\nabla^*$.

It is discretized, for $v=(v_1,v_2)$ as 

\[
\diver(v)_i = v^1_{i_1+1,i_2} - v^1_{i_1,i_2} + v^2_{i_1,i_2+1} - v^2_{i_1,i_2}
\]

It is a local measure of its \textit{``outgoingness''}, i.e. the extent to which there is more of the field vectors exiting an infinitesimal region of space than entering it. A point at which the flux is outgoing has positive divergence, and is often called a \textit{``source''} of the field. A point at which the flux is directed inward has negative divergence, and is often called a \textit{``sink''} of the field. The greater the flux of field through a small surface enclosing a given point, the greater the value of divergence at that point. A point at which there is zero flux through an enclosing surface has zero divergence.

\paragraph{Laplacian Operator} The Laplacian operator is defined as $\Delta = \diver\circ\nabla = -\nabla^*\circ\nabla$. That is, the divergence of the gradient and denotes the rate at which the average value of an image $g$ over spheres centered at $s$ deviates from $g(s)$ as the radius of the spheres shrinks towards $0$. Basically it is the sum of the second order partial derivatives.

Figure \ref{fig:grad_lap} shows the differences between the gradient and the Laplacian operators in our sample image.

\begin{figure}[htbp]
	\centering 
		\includegraphics[width = 0.65\textwidth]{../figures/grad_lap.pdf}
	\caption{Gradient, and Laplacian Operators}
	\label{fig:grad_lap}
\end{figure}

\section{Newton's Method for Unconstrained Problems}

Let us start with a highly anisotropic function, the Rosenbrock function given by
\[
g(x) = (1-x_1)^2 + 100(x_2-x_1^2)^2
\]
where at $x^*=(1,1)$ the function reaches its minimum with $g(x^*) = 0$. Figure \ref{fig:rosenbrock} shows the behavior of the function on a $3-$D grid.

\begin{figure}[htbp]
	\centering
		\includegraphics[width = 0.55\textwidth]{../figures/rosenbrock.pdf}
	\caption{Rosenbrock Function}
	\label{fig:rosenbrock}
\end{figure}

If we would use gradient descent methods that only use first order gradient information about $g$, we would not be able to efficiently minimize the function because of the high anisotropy. Defining the gradient of $g$ as
\[
\nabla g(x) = \left(\frac{\partial g(x)}{\partial x_1},\frac{\partial g(x)}{\partial x_2}\right) = \left(2(x_1-1) + 400 x_1(x_1^2-x_2),200(x_2-x_1^2)\right)\in\mathbb{R}^2
\]

The Hessian matrix can be computed as
\[
\mathcal{H}g(x) = \begin{pmatrix}
\frac{\partial^2 g(x)}{\partial x_1^2} & \frac{\partial^2 g(x)}{\partial x_1\partial x_2} \\
\frac{\partial^2 g(x)}{\partial x_1 \partial x_2} & \frac{\partial^2 g(x)}{\partial x_2^2}
\end{pmatrix} = \begin{pmatrix}
2 + 400 (x_1^2-x_2) + 800 x_1^2 & -400 x_1 \\
-400 x_1 & 200
\end{pmatrix} \in \mathbb{R}^{2\times 2}
\]

The Newton descent method starting from some $x^{(0)}\in\mathbb{R}^2$ is computed by iterating on
\[
x^{(\ell+1)} = x^{(\ell)} - \left[\mathcal{H}g\left(x^{(\ell)}\right)\right]^{-1}\nabla g\left(x^{(\ell)}\right)
\]

\paragraph{Intuition of Newton's Method in 1 Dimension} Suppose we have a single variable function $f(x)$, the method tries to find the roots of $f'(x)$ by constructing a sequence $x^{(\ell)}$ from an initial guess $x^{(0)}$ that converges towards some value $x^*$ satisfying $f'(x^*) = 0$. Taking a second-order Taylor expansion $f_T(x)$ of $f(x)$ around $x^{(\ell)}$:
\[
f_T(x) = f_T(x^{(\ell)} + \Delta x) \approx f(x^{(\ell)}) + f'(x^{(\ell)})\Delta x + \frac{1}{2}f''(x^{(\ell)})\Delta x^2
\]

We want to pick $\Delta x$ such that $x^{(\ell)} + \Delta x$ is a stationary point of $f$. Using the Taylor expansion, we can solve for $\Delta x$ corresponding to the root of the expansion's derivative:
\[
\Delta x = -\frac{f'(x^{(\ell)})}{f''(x^{(\ell)})}
\]

Provided the Taylor expansion approximation is fairly accurate, then incrementing by $\Delta x$ should yield a point close enough to an actual stationary point of $f$. This point is given by 
\[
x^{(\ell+1)} = x^{(\ell)} + \Delta x = x^{(\ell)} - \frac{f'(x^{(\ell)})}{f''(x^{(\ell)})}
\]

For a two-dimensional problem, we just replace the first derivative by the gradient $\nabla f(x)$ and the reciprocal of the second derivative by the inverse of the Hessian matrix $\left[\mathcal{H}g\left(x^{(\ell)}\right)\right]^{-1}$.

\end{document}