%========================================================================================
% Compilation should work with PDFLaTeX
%========================================================================================
% Type of document and general formatting
\documentclass[a4paper,11pt]{article}

\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\linespread{1.25}

%========================================================================================
% These packages are for language and font settings
\usepackage[english,activeacute]{babel} % Language
\usepackage{tgpagella}					% Text font
\usepackage[T1]{fontenc}				% T1 Encoding of font
\usepackage[utf8x]{inputenc}				% Special symbols
\usepackage{lmodern}


%========================================================================================
\usepackage[sc]{mathpazo}				% Math font
\usepackage{amsmath,amsfonts,amssymb}	% Math symbols
\usepackage{dsfont}						% Math symbols like R for reals...


%========================================================================================
% Other packages
\usepackage{graphicx}
\usepackage{longtable}
\usepackage[svgnames]{xcolor}

%========================================================================================
\usepackage{accents}
\newcommand*{\dt}[1]{%
	\accentset{\mbox{\large\bfseries .}}{#1}} % Larger dot for time derivative


\usepackage{hyperref}
\hypersetup
{
    pdfauthor={Rafael Serrano-Quintero},
    pdfsubject={Structural Transformation in Indian Services},
    colorlinks = {true},
    linkcolor = {FireBrick},
    citecolor = {FireBrick},
    urlcolor = {RoyalBlue},
}

\usepackage{appendix}
\usepackage{marvosym}
\usepackage{enumerate} %For enumerating with letters with option [a)]
\usepackage{fancyvrb}  %To reduce font size in verbatim environment
\usepackage{epstopdf}
\usepackage[flushleft]{threeparttable}
\usepackage{pdflscape}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage[super]{nth}
\usepackage{float}

\newcommand{\source}[1]{\caption*{\tiny Source: {#1}} }

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\dotprod}[2]{\langle #1, #2 \rangle}

%========================================================================================
% Stata Preamble for Tables
%========================================================================================

\newcommand{\sym}[1]{\rlap{#1}}% Thanks to David Carlisle

\let\estinput=\input% define a new input command so that we can still flatten the document

\newcommand{\estwide}[3]{
		\vspace{.75ex}{
			\begin{tabular*}
			{\textwidth}{@{\hskip\tabcolsep\extracolsep\fill}l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
			\end{tabular*}
			}
		}	

\newcommand{\estauto}[3]{
		\vspace{.75ex}{
			\begin{tabular}{l*{#2}{#3}}
			\toprule
			\estinput{#1}
			\bottomrule
			\addlinespace[.75ex]
			\end{tabular}
			}
		}

% Allow line breaks with \\ in specialcells
	\newcommand{\specialcell}[2][c]{%
	\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% *****************************************************************
% Custom subcaptions
% *****************************************************************
% Note/Source/Text after Tables
\newcommand{\figtext}[1]{
	\vspace{-1.9ex}
	\captionsetup{justification=justified,font=footnotesize}
	\caption*{\hspace{6pt}\hangindent=1.5em #1}
	}
\newcommand{\fignote}[1]{\figtext{\emph{Note:~}~#1}}

\newcommand{\figsource}[1]{\figtext{\emph{Source:~}~#1}}

% Add significance note with \starnote
\newcommand{\starnote}{\figtext{* p < 0.1, ** p < 0.05, *** p < 0.01. Standard errors in parentheses.}}

% *****************************************************************
% siunitx
% *****************************************************************
\usepackage{siunitx} % centering in tables
	\sisetup{
		detect-mode,
		tight-spacing		= true,
		group-digits		= false ,
		input-signs		= ,
		input-symbols		= ( ) [ ] - + *,
		input-open-uncertainty	= ,
		input-close-uncertainty	= ,
		table-align-text-post	= false
        }

%========================================================================================
					% === Title, thanks, and author data === %
%========================================================================================


\title{\textbf{Optimization Methods}\thanks{These are notes based on Gabriel Peyr\'e's exceptional \href{http://www.numerical-tours.com/matlab/}{Numerical Tours} and are for my own personal study.}}

\begin{document}
\maketitle

\section{Gradient Descent Methods}

\subsection{Gradient Descent for Unconstrained Problems}

We consider the problem of finding a minimum of a function $f$, hence solving

\[
\underset{x\in\mathbb{R}^d}{\text{min}} f(x)
\]

where $f : \mathbb{R}^d \mapsto \mathbb{R}$ is a smooth function.

The minimum is not necessarily unique. In the general case, $f$ might exhibit local minima, in which case the proposed algorithms are not expected to find a global minimizer of the problem. In this tour, we restrict our attention to convex function, so that the methods will converge to a global minimizer.

The simplest method is the gradient descent, that computes

\[
x^{(k+1)} = x^{(k)} - \tau_k \nabla f\left(x^{(k)}\right)
\]

where $\tau_k > 0$ is a step size, and $\nabla f(x) \in\mathbb{R}^d$ is the gradient of $f$ at the point $x$, and $x^{(0)}\in\mathbb{R}^d$ is an initial point.

In the convex case, if $f$ is of class $\mathcal{C}^2$, in order to ensure convergence, the step size should satisfy 

\[
0 < \tau_k < \frac{2}{\text{sup}_x \norm{H f(x)}}
\]

where $Hf(x)\in\mathbb{R}^{d\times d}$ is the Hessian of $f$ at $x$ and $\norm{\cdot}$ is the spectral operator norm (largest eigenvalue). 

The following code takes $f(x) = x^2$, computes the gradient manually and applies  gradient descent to get to the solution $x = 0$.

\begin{Verbatim}[fontsize=\small]
tau = 2e-1; 			% Step-size parameter
f = @(x)(x.^2);			% Function to minimize
fgrad = @(x)(2.*x);		% Gradient

x0 = -500;		% Initial guess
tol = 1e-6;		% Tolerance of the algorithm
err = 10000;	% Initial error
it = 1;			% Iteration counter

while err > tol
	fgradx0 = fgrad(x0);
	x1(it+1) = x0-tau.*fgradx0;
	err = abs(fgradx0); % Error is the absolute value of the gradient
	if err > tol
		x0 = x1(it+1);
	end
	fprintf('New x = %3.3f \n',x0)
	it = it+1;
end
\end{Verbatim}

\subsection{Gradient Descent in 2-D}
Suppose we want to minimize the following quadratic form

\[
f(x) = \frac{1}{2}\left(x_1^2 + \eta x_2^2\right)
\]

where $\eta$ controls the anisotropy and, hence, the difficulty of the problem.\footnote{Anisotropy is the property of being directionally dependent, which implies different properties in different directions, as opposed to isotropy.} Let us set $\eta = 10$.

The rationale for the code is the same as before. However, now I impose two stopping conditions, one for each coordinate. Note that the step-size parameter $\tau_k$ needs to be smaller than $2/\eta$. Figure

\begin{figure}[htbp]
	\centering 
		\includegraphics[width = 0.35\textwidth]{../figures/gradient_descent_2D.pdf}
	\caption{Visualization of Gradient Descent in $2-$D for Different Values of $\tau_k$}
	\label{fig:gradient_descent_2d}
\end{figure}

\subsection{Gradient and Divergence of Images}
Local differential operators like gradient, divergence and laplacian are the building blocks for variational image processing.

An image is a matrix $x_0\in\mathbb{R}^N$ of $N=n\times n$ pixels. For a continuous function $g$, the gradient reads

\[
\nabla g(s) = \left(\frac{\partial g(s)}{\partial s_1},\frac{\partial g(s)}{\partial s_2}\right)\in\mathbb{R}^2
\]
(note that here, the variable $s$ denotes the $2-$D spatial position).

We discretize this differential operator on a discrete image $x\in\mathbb{R}^N$ using first order finite differences.

\[
\left(\nabla x\right)_i = (x_{i_1,i_2} - x_{i_1 - 1, i_2}, x_{i_1,i_2} - x_{i_1,i_2-1})\in\mathbb{R}^2
\]

Note that for simplity we use periodic boundary conditions. Thus, we get $\nabla : \mathbb{R}^n\mapsto\mathbb{R}^{N\times 2}$. Figure \ref{fig:grad_image} shows the discretized gradient on an image. The left-hand side shows the first component of the gradient, while the right-hand side one shows the second component of the gradient.

\begin{figure}[htbp]
	\centering
		\includegraphics[width = 0.45\textwidth]{../figures/gradient_descent_image.pdf}
	\caption{Gradient Descent in a 2-D Image}
	\label{fig:grad_image}
\end{figure}

The divergence operator maps vector field to images. For continuous vector fields $v(s)\in\mathbb{R}^2$, it is defined as 
\[
\text{div}(v)(s) = \frac{\partial v_1(s)}{\partial s_1} + \frac{\partial v_2(s)}{\partial s_2} \in\mathbb{R}
\]

(note that here, the variable $s$ denotes the 2-D spacial position). It is minus the adjoint of the gadient, i.e. $\text{div}=âˆ’\nabla^{\text{\textasteriskcentered}}$.

It is discretized, for $v=(v_1,v_2)$ as 

\[
\text{div}(v)_i = v^1_{i_1+1,i_2} - v^1_{i_1,i_2} + v^2_{i_1,i_2+1} - v^2_{i_1,i_2}
\]


\end{document}